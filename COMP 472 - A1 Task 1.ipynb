{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf57f80",
   "metadata": {},
   "source": [
    "#### Fall 2021 | COMP 4721"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259843f",
   "metadata": {},
   "source": [
    "## Mini-Project 1\n",
    "\n",
    "#### Wei Chen Huang, Ian Phillips, Beatrice Cobo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d8fc2",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "import statements for necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ffb4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from pandas import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef37910",
   "metadata": {},
   "source": [
    "2. Plot the distribution of the instances in each class and save the graphic in a file called BBC-distribution.pdf.\n",
    "You may want to use matplotlib.pyplot and savefig to do this. This pre-analysis of the data set will\n",
    "allow you to determine if the classes are balanced, and which metric is more appropriate to use to evaluate\n",
    "the performance of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b58c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datadir=\"BBC\"\n",
    "categories=[\"tech\", \"entertainment\", \"politics\", \"business\", \"sport\"]\n",
    "txt_array=[];\n",
    "all_array=[];\n",
    "for category in categories:  # for each class\n",
    "    path = os.path.join(datadir,category)  # create path to class\n",
    "    for txt in os.listdir(path):  # iterate over each text file per class\n",
    "        txt_array.append(txt)\n",
    "    all_array.append(len(txt_array))\n",
    "    txt_array=[]\n",
    "fig = plt.figure()\n",
    "plt.bar(categories,all_array)\n",
    "fig.savefig('BBC-distribution.pdf', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d3f1a",
   "metadata": {},
   "source": [
    "3. Load the corpus using load files and make sure you set the encoding to latin1. This will read the file\n",
    "structure and assign the category name to each file from their parent directory name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c608f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=load_files(container_path=datadir,encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b379d1",
   "metadata": {},
   "source": [
    "4. Pre-process the dataset to have the features ready to be used by a multinomial Naive Bayes classifier. This\n",
    "means that the frequency of each word in each class must be computed and stored in a term-document\n",
    "matrix. For this, you can use feature extraction.text.CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5705c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12f7c8",
   "metadata": {},
   "source": [
    "5. Split the dataset into 80% for training and 20% for testing. For this, you must use train test split with\n",
    "the parameter random state set to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f00608",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,corpus.target, test_size=0.2,random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd106d",
   "metadata": {},
   "source": [
    "6. Train a multinomial Naive Bayes Classifier (naive bayes.MultinomialNB) on the training set using the\n",
    "default parameters and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "decf128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b129741",
   "metadata": {},
   "source": [
    "7. In a file called bbc-performance.txt, save the following information: (to make it easier for the TAs, make\n",
    "sure that your output for each sub-question below is clearly marked in your output file, using the headings\n",
    "(a), (b) . . .)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918c135",
   "metadata": {},
   "source": [
    "(b) the confusion matrix (you can use confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30d7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix = confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dba303",
   "metadata": {},
   "source": [
    "(c) the precision, recall, and F1-measure for each class (you can use classification report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa843b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.97      0.95      0.96       115\n",
      "entertainment       1.00      0.90      0.95        68\n",
      "     politics       0.93      0.97      0.95        80\n",
      "        sport       1.00      1.00      1.00       101\n",
      "         tech       0.92      0.99      0.95        81\n",
      "\n",
      "     accuracy                           0.96       445\n",
      "    macro avg       0.96      0.96      0.96       445\n",
      " weighted avg       0.97      0.96      0.96       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classificationReport = classification_report(y_test, predicted, target_names=corpus.target_names)\n",
    "print(classificationReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dffad5",
   "metadata": {},
   "source": [
    "(d) the accuracy, macro-average F1 and weighted-average F1 of the model (you can use accuracy score\n",
    "and f1 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "950064eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyScore = accuracy_score(y_test, predicted)\n",
    "macroF1Score = f1_score(y_test, predicted, average='macro')\n",
    "weightedF1Score = f1_score(y_test, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb36976",
   "metadata": {},
   "source": [
    "(e) the prior probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6bc1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = [x / (len(corpus.target)) for x in all_array]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d22e5",
   "metadata": {},
   "source": [
    "(f) the size of the vocabulary (i.e. the number of different words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8271514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularyLength = len((vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18404c58",
   "metadata": {},
   "source": [
    "(g) the number of word-tokens in each class (i.e. the number of words in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a83f4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70596903\n",
      "68146975\n",
      "62938590\n",
      "55867735\n",
      "73174753\n"
     ]
    }
   ],
   "source": [
    "enter_arr=[]\n",
    "tech_arr=[]\n",
    "sport_arr=[]\n",
    "politics_arr=[]\n",
    "business_arr=[]\n",
    "i = 0\n",
    "\n",
    "arr={'business':[],'entertainment':[],'politics':[],'sport':[],'tech':[]}\n",
    "    \n",
    "for data in corpus.target:  # for each class\n",
    "    if(data==0):\n",
    "        arr['business'].append(corpus.data[i])\n",
    "    if(data==1):\n",
    "        arr['entertainment'].append(corpus.data[i])\n",
    "    if(data==2):\n",
    "        arr['politics'].append(corpus.data[i])\n",
    "    if(data==3):\n",
    "        arr['sport'].append(corpus.data[i])\n",
    "    if(data==4):\n",
    "        arr['tech'].append(corpus.data[i])\n",
    "    i=i+1;\n",
    "\n",
    "total_business=0\n",
    "total_entertainment=0\n",
    "total_politics=0\n",
    "total_sport=0\n",
    "total_tech=0\n",
    "\n",
    "vectorizer2 = CountVectorizer()\n",
    "X = vectorizer2.fit_transform(arr['business'])\n",
    "business_arr=vectorizer2.vocabulary_\n",
    "for b in business_arr:\n",
    "    total_business+=vectorizer2.vocabulary_.get(b)\n",
    "\n",
    "vectorizer2 = CountVectorizer()\n",
    "X = vectorizer2.fit_transform(arr['entertainment'])\n",
    "entertainment_arr=vectorizer2.vocabulary_\n",
    "for e in entertainment_arr:\n",
    "    total_entertainment+=vectorizer2.vocabulary_.get(e)\n",
    "    \n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['politics'])\n",
    "politics_arr=vectorizer2.vocabulary_\n",
    "for p in politics_arr:\n",
    "    total_politics+=vectorizer2.vocabulary_.get(p)\n",
    "    \n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['sport'])\n",
    "sport_arr=vectorizer2.vocabulary_\n",
    "for s in sport_arr:\n",
    "    total_sport+=vectorizer2.vocabulary_.get(s)\n",
    "\n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['tech'])\n",
    "tech_arr=vectorizer2.vocabulary_\n",
    "for t in tech_arr:\n",
    "    total_tech+=vectorizer2.vocabulary_.get(t)\n",
    "\n",
    "print(total_business)\n",
    "print(total_entertainment)\n",
    "print(total_politics)\n",
    "print(total_sport)\n",
    "print(total_tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0960ffe",
   "metadata": {},
   "source": [
    "(h) the number of word-tokens in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056718f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bd040ca",
   "metadata": {},
   "source": [
    "(i) the number and percentage of words with a frequency of zero in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f2ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17407d99",
   "metadata": {},
   "source": [
    "(j) the number and percentage of words with a frequency of one in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb0026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a863dd41",
   "metadata": {},
   "source": [
    "(k) your 2 favorite words (that are present in the vocabulary) and their log-prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcd12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd07716e",
   "metadata": {},
   "source": [
    "Write to `bbc-performance.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d355f9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('bbc-performance.txt', 'w')\n",
    "f.write('MultinomialNB default values, try 1\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('b) Confusion matrix\\n')\n",
    "f.write(DataFrame(cmatrix).to_string() + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('c) Classification Report\\n\\n')\n",
    "f.write(classificationReport + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('d) Accuracy, Macro Average F1, Weighted Average\\n')\n",
    "f.write('accuracy: ' + str(accuracyScore) + '\\n')\n",
    "f.write('macro average f1: ' + str(macroF1Score) + '\\n')\n",
    "f.write('weighted average: ' + str(weightedF1Score) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('e) Prior Probabilities\\n')\n",
    "f.write('business: ' + str(priors[3]) + '\\n')\n",
    "f.write('entertainment: ' + str(priors[1]) + '\\n')\n",
    "f.write('politics: ' + str(priors[2]) + '\\n')\n",
    "f.write('sports: ' + str(priors[4]) + '\\n')\n",
    "f.write('tech: ' + str(priors[0]) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('f) Size of vocabulary\\n')\n",
    "f.write('size of vocabulary: ' + str(vocabularyLength) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('g) Number of words-tokens in each class\\n')\n",
    "f.write('business: ' + str(total_business) + '\\n')\n",
    "f.write('entertainment: ' + str(total_entertainment) + '\\n')\n",
    "f.write('politics: ' + str(total_politics) + '\\n')\n",
    "f.write('sport: ' + str(total_sport) + '\\n')\n",
    "f.write('tech: ' + str(total_tech) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('h) Number of words-tokens in entire corpus\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('i) Number and percentage of words with a frequency of zero in each class\\n')\n",
    "f.write('business: ' + '\\n')\n",
    "f.write('entertainment: ' + '\\n')\n",
    "f.write('politics: ' + '\\n')\n",
    "f.write('sport: ' + '\\n')\n",
    "f.write('tech: ' + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('j) Number and percentage of words with a frequency of one in the entire corpus\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('k) Our 2 favorite words and their log-prob \\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac3a81",
   "metadata": {},
   "source": [
    "8. Redo steps 6 and 7 without changing anything (do not redo step 5, the dataset split). Change the\n",
    "model name to something like “MultinomialNB default values, try 2” and append the results to the file\n",
    "bbc-performance.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14dd21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab834153",
   "metadata": {},
   "source": [
    "(b) the confusion matrix (you can use confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9a3dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix = confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d6856",
   "metadata": {},
   "source": [
    "(c) the precision, recall, and F1-measure for each class (you can use classification report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ab9d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.97      0.95      0.96       115\n",
      "entertainment       1.00      0.90      0.95        68\n",
      "     politics       0.93      0.97      0.95        80\n",
      "        sport       1.00      1.00      1.00       101\n",
      "         tech       0.92      0.99      0.95        81\n",
      "\n",
      "     accuracy                           0.96       445\n",
      "    macro avg       0.96      0.96      0.96       445\n",
      " weighted avg       0.97      0.96      0.96       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classificationReport = classification_report(y_test, predicted, target_names=corpus.target_names)\n",
    "print(classificationReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf1d93",
   "metadata": {},
   "source": [
    "(d) the accuracy, macro-average F1 and weighted-average F1 of the model (you can use accuracy score\n",
    "and f1 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b05304",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyScore = accuracy_score(y_test, predicted)\n",
    "macroF1Score = f1_score(y_test, predicted, average='macro')\n",
    "weightedF1Score = f1_score(y_test, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace70b0",
   "metadata": {},
   "source": [
    "(e) the prior probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a02d2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = [x / (len(corpus.target)) for x in all_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb351fd5",
   "metadata": {},
   "source": [
    "(f) the size of the vocabulary (i.e. the number of different words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c956d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularyLength = len((vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79116654",
   "metadata": {},
   "source": [
    "(g) the number of word-tokens in each class (i.e. the number of words in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eea2e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70596903\n",
      "68146975\n",
      "62938590\n",
      "55867735\n",
      "73174753\n"
     ]
    }
   ],
   "source": [
    "enter_arr=[]\n",
    "tech_arr=[]\n",
    "sport_arr=[]\n",
    "politics_arr=[]\n",
    "business_arr=[]\n",
    "i = 0\n",
    "\n",
    "arr={'business':[],'entertainment':[],'politics':[],'sport':[],'tech':[]}\n",
    "    \n",
    "for data in corpus.target:  # for each class\n",
    "    if(data==0):\n",
    "        arr['business'].append(corpus.data[i])\n",
    "    if(data==1):\n",
    "        arr['entertainment'].append(corpus.data[i])\n",
    "    if(data==2):\n",
    "        arr['politics'].append(corpus.data[i])\n",
    "    if(data==3):\n",
    "        arr['sport'].append(corpus.data[i])\n",
    "    if(data==4):\n",
    "        arr['tech'].append(corpus.data[i])\n",
    "    i=i+1;\n",
    "\n",
    "total_business=0\n",
    "total_entertainment=0\n",
    "total_politics=0\n",
    "total_sport=0\n",
    "total_tech=0\n",
    "\n",
    "vectorizer2 = CountVectorizer()\n",
    "X = vectorizer2.fit_transform(arr['business'])\n",
    "business_arr=vectorizer2.vocabulary_\n",
    "for b in business_arr:\n",
    "    total_business+=vectorizer2.vocabulary_.get(b)\n",
    "\n",
    "vectorizer2 = CountVectorizer()\n",
    "X = vectorizer2.fit_transform(arr['entertainment'])\n",
    "entertainment_arr=vectorizer2.vocabulary_\n",
    "for e in entertainment_arr:\n",
    "    total_entertainment+=vectorizer2.vocabulary_.get(e)\n",
    "    \n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['politics'])\n",
    "politics_arr=vectorizer2.vocabulary_\n",
    "for p in politics_arr:\n",
    "    total_politics+=vectorizer2.vocabulary_.get(p)\n",
    "    \n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['sport'])\n",
    "sport_arr=vectorizer2.vocabulary_\n",
    "for s in sport_arr:\n",
    "    total_sport+=vectorizer2.vocabulary_.get(s)\n",
    "\n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['tech'])\n",
    "tech_arr=vectorizer2.vocabulary_\n",
    "for t in tech_arr:\n",
    "    total_tech+=vectorizer2.vocabulary_.get(t)\n",
    "\n",
    "print(total_business)\n",
    "print(total_entertainment)\n",
    "print(total_politics)\n",
    "print(total_sport)\n",
    "print(total_tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f184939",
   "metadata": {},
   "source": [
    "(h) the number of word-tokens in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8515b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a2acc88",
   "metadata": {},
   "source": [
    "(i) the number and percentage of words with a frequency of zero in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ddb192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1883971",
   "metadata": {},
   "source": [
    "(j) the number and percentage of words with a frequency of one in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cde449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de9d9213",
   "metadata": {},
   "source": [
    "(k) your 2 favorite words (that are present in the vocabulary) and their log-prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ed4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e38aad5",
   "metadata": {},
   "source": [
    "Write to `bbc-performance.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "145992e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.write('\\n')\n",
    "f.write('\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('\\n')\n",
    "f.write('\\n')\n",
    "f.write('MultinomialNB default values, try 2\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('b) Confusion matrix\\n')\n",
    "f.write(DataFrame(cmatrix).to_string() + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('c) Classification Report\\n\\n')\n",
    "f.write(classificationReport + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('d) Accuracy, Macro Average F1, Weighted Average\\n')\n",
    "f.write('accuracy: ' + str(accuracyScore) + '\\n')\n",
    "f.write('macro average f1: ' + str(macroF1Score) + '\\n')\n",
    "f.write('weighted average: ' + str(weightedF1Score) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('e) Prior Probabilities\\n')\n",
    "f.write('business: ' + str(priors[3]) + '\\n')\n",
    "f.write('entertainment: ' + str(priors[1]) + '\\n')\n",
    "f.write('politics: ' + str(priors[2]) + '\\n')\n",
    "f.write('sports: ' + str(priors[4]) + '\\n')\n",
    "f.write('tech: ' + str(priors[0]) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('f) Size of vocabulary\\n')\n",
    "f.write('size of vocabulary: ' + str(vocabularyLength) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('g) Number of words-tokens in each class\\n')\n",
    "f.write('business: ' + str(total_business) + '\\n')\n",
    "f.write('entertainment: ' + str(total_entertainment) + '\\n')\n",
    "f.write('politics: ' + str(total_politics) + '\\n')\n",
    "f.write('sport: ' + str(total_sport) + '\\n')\n",
    "f.write('tech: ' + str(total_tech) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('h) Number of words-tokens in entire corpus\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('i) Number and percentage of words with a frequency of zero in each class\\n')\n",
    "f.write('business: ' + '\\n')\n",
    "f.write('entertainment: ' + '\\n')\n",
    "f.write('politics: ' + '\\n')\n",
    "f.write('sport: ' + '\\n')\n",
    "f.write('tech: ' + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('j) Number and percentage of words with a frequency of one in the entire corpus\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('k) Our 2 favorite words and their log-prob \\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448b9fc",
   "metadata": {},
   "source": [
    "9. Redo steps 6 and 7 again, but this time, change the smoothing value to 0.0001. Append the results at the\n",
    "end of bbc-performance.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ffc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "smoothing_param = {'alpha': 0.0001}\n",
    "clf.set_params(**smoothing_param)\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266dc05",
   "metadata": {},
   "source": [
    "(b) the confusion matrix (you can use confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2591b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix = confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d301e4",
   "metadata": {},
   "source": [
    "(c) the precision, recall, and F1-measure for each class (you can use classification report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44bdc816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.98      0.95      0.96       115\n",
      "entertainment       1.00      0.91      0.95        68\n",
      "     politics       0.93      0.99      0.96        80\n",
      "        sport       1.00      1.00      1.00       101\n",
      "         tech       0.93      0.99      0.96        81\n",
      "\n",
      "     accuracy                           0.97       445\n",
      "    macro avg       0.97      0.97      0.97       445\n",
      " weighted avg       0.97      0.97      0.97       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classificationReport = classification_report(y_test, predicted, target_names=corpus.target_names)\n",
    "print(classificationReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c45990",
   "metadata": {},
   "source": [
    "(d) the accuracy, macro-average F1 and weighted-average F1 of the model (you can use accuracy score\n",
    "and f1 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "830a83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyScore = accuracy_score(y_test, predicted)\n",
    "macroF1Score = f1_score(y_test, predicted, average='macro')\n",
    "weightedF1Score = f1_score(y_test, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3798854",
   "metadata": {},
   "source": [
    "(e) the prior probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "785a95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = [x / (len(corpus.target)) for x in all_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2ffbe",
   "metadata": {},
   "source": [
    "(f) the size of the vocabulary (i.e. the number of different words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fa384db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularyLength = len((vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95990df",
   "metadata": {},
   "source": [
    "(g) the number of word-tokens in each class (i.e. the number of words in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0217882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70596903\n",
      "68146975\n",
      "62938590\n",
      "55867735\n",
      "73174753\n"
     ]
    }
   ],
   "source": [
    "enter_arr=[]\n",
    "tech_arr=[]\n",
    "sport_arr=[]\n",
    "politics_arr=[]\n",
    "business_arr=[]\n",
    "i = 0\n",
    "\n",
    "arr={'business':[],'entertainment':[],'politics':[],'sport':[],'tech':[]}\n",
    "    \n",
    "for data in corpus.target:  # for each class\n",
    "    if(data==0):\n",
    "        arr['business'].append(corpus.data[i])\n",
    "    if(data==1):\n",
    "        arr['entertainment'].append(corpus.data[i])\n",
    "    if(data==2):\n",
    "        arr['politics'].append(corpus.data[i])\n",
    "    if(data==3):\n",
    "        arr['sport'].append(corpus.data[i])\n",
    "    if(data==4):\n",
    "        arr['tech'].append(corpus.data[i])\n",
    "    i=i+1;\n",
    "\n",
    "total_business=0\n",
    "total_entertainment=0\n",
    "total_politics=0\n",
    "total_sport=0\n",
    "total_tech=0\n",
    "\n",
    "vectorizer2 = CountVectorizer()\n",
    "X = vectorizer2.fit_transform(arr['business'])\n",
    "business_arr=vectorizer2.vocabulary_\n",
    "for b in business_arr:\n",
    "    total_business+=vectorizer2.vocabulary_.get(b)\n",
    "\n",
    "vectorizer2 = CountVectorizer()\n",
    "X = vectorizer2.fit_transform(arr['entertainment'])\n",
    "entertainment_arr=vectorizer2.vocabulary_\n",
    "for e in entertainment_arr:\n",
    "    total_entertainment+=vectorizer2.vocabulary_.get(e)\n",
    "    \n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['politics'])\n",
    "politics_arr=vectorizer2.vocabulary_\n",
    "for p in politics_arr:\n",
    "    total_politics+=vectorizer2.vocabulary_.get(p)\n",
    "    \n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['sport'])\n",
    "sport_arr=vectorizer2.vocabulary_\n",
    "for s in sport_arr:\n",
    "    total_sport+=vectorizer2.vocabulary_.get(s)\n",
    "\n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['tech'])\n",
    "tech_arr=vectorizer2.vocabulary_\n",
    "for t in tech_arr:\n",
    "    total_tech+=vectorizer2.vocabulary_.get(t)\n",
    "\n",
    "print(total_business)\n",
    "print(total_entertainment)\n",
    "print(total_politics)\n",
    "print(total_sport)\n",
    "print(total_tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18a31b",
   "metadata": {},
   "source": [
    "(h) the number of word-tokens in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709cd872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a254b7e",
   "metadata": {},
   "source": [
    "(i) the number and percentage of words with a frequency of zero in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09423b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aff4ced1",
   "metadata": {},
   "source": [
    "(j) the number and percentage of words with a frequency of one in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e61264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5bcfbf0",
   "metadata": {},
   "source": [
    "(k) your 2 favorite words (that are present in the vocabulary) and their log-prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864dcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3ca051d",
   "metadata": {},
   "source": [
    "Write to `bbc-performance.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "266af858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.write('\\n')\n",
    "f.write('\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('\\n')\n",
    "f.write('\\n')\n",
    "f.write('MultinomialNB with smoothing = 0.0001\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('b) Confusion matrix\\n')\n",
    "f.write(DataFrame(cmatrix).to_string() + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('c) Classification Report\\n\\n')\n",
    "f.write(classificationReport + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('d) Accuracy, Macro Average F1, Weighted Average\\n')\n",
    "f.write('accuracy: ' + str(accuracyScore) + '\\n')\n",
    "f.write('macro average f1: ' + str(macroF1Score) + '\\n')\n",
    "f.write('weighted average: ' + str(weightedF1Score) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('e) Prior Probabilities\\n')\n",
    "f.write('business: ' + str(priors[3]) + '\\n')\n",
    "f.write('entertainment: ' + str(priors[1]) + '\\n')\n",
    "f.write('politics: ' + str(priors[2]) + '\\n')\n",
    "f.write('sports: ' + str(priors[4]) + '\\n')\n",
    "f.write('tech: ' + str(priors[0]) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('f) Size of vocabulary\\n')\n",
    "f.write('size of vocabulary: ' + str(vocabularyLength) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('g) Number of words-tokens in each class\\n')\n",
    "f.write('business: ' + str(total_business) + '\\n')\n",
    "f.write('entertainment: ' + str(total_entertainment) + '\\n')\n",
    "f.write('politics: ' + str(total_politics) + '\\n')\n",
    "f.write('sport: ' + str(total_sport) + '\\n')\n",
    "f.write('tech: ' + str(total_tech) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('h) Number of words-tokens in entire corpus\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('i) Number and percentage of words with a frequency of zero in each class\\n')\n",
    "f.write('business: ' + '\\n')\n",
    "f.write('entertainment: ' + '\\n')\n",
    "f.write('politics: ' + '\\n')\n",
    "f.write('sport: ' + '\\n')\n",
    "f.write('tech: ' + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('j) Number and percentage of words with a frequency of one in the entire corpus\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('k) Our 2 favorite words and their log-prob \\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dda62c",
   "metadata": {},
   "source": [
    "10. Redo steps 6 and 7, but this time, change the smoothing value to 0.9. Append the results at the end of\n",
    "bbc-performance.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc633e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "smoothing_param = {'alpha': 0.9}\n",
    "clf.set_params(**smoothing_param)\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366214b",
   "metadata": {},
   "source": [
    "(b) the confusion matrix (you can use confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a762351",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix = confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0909b",
   "metadata": {},
   "source": [
    "(c) the precision, recall, and F1-measure for each class (you can use classification report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca541646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.97      0.95      0.96       115\n",
      "entertainment       1.00      0.91      0.95        68\n",
      "     politics       0.94      0.97      0.96        80\n",
      "        sport       1.00      1.00      1.00       101\n",
      "         tech       0.92      0.99      0.95        81\n",
      "\n",
      "     accuracy                           0.97       445\n",
      "    macro avg       0.97      0.96      0.96       445\n",
      " weighted avg       0.97      0.97      0.97       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classificationReport = classification_report(y_test, predicted, target_names=corpus.target_names)\n",
    "print(classificationReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ec2c4",
   "metadata": {},
   "source": [
    "(d) the accuracy, macro-average F1 and weighted-average F1 of the model (you can use accuracy score\n",
    "and f1 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5aa6e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyScore = accuracy_score(y_test, predicted)\n",
    "macroF1Score = f1_score(y_test, predicted, average='macro')\n",
    "weightedF1Score = f1_score(y_test, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e30ec1e",
   "metadata": {},
   "source": [
    "(e) the prior probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f904e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = [x / (len(corpus.target)) for x in all_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc044ff3",
   "metadata": {},
   "source": [
    "(f) the size of the vocabulary (i.e. the number of different words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa5a0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularyLength = len((vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc44ba2",
   "metadata": {},
   "source": [
    "(g) the number of word-tokens in each class (i.e. the number of words in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5f6f646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70596903\n",
      "68146975\n",
      "62938590\n",
      "55867735\n",
      "73174753\n"
     ]
    }
   ],
   "source": [
    "enter_arr=[]\n",
    "tech_arr=[]\n",
    "sport_arr=[]\n",
    "politics_arr=[]\n",
    "business_arr=[]\n",
    "i = 0\n",
    "\n",
    "arr={'business':[],'entertainment':[],'politics':[],'sport':[],'tech':[]}\n",
    "    \n",
    "for data in corpus.target:  # for each class\n",
    "    if(data==0):\n",
    "        arr['business'].append(corpus.data[i])\n",
    "    if(data==1):\n",
    "        arr['entertainment'].append(corpus.data[i])\n",
    "    if(data==2):\n",
    "        arr['politics'].append(corpus.data[i])\n",
    "    if(data==3):\n",
    "        arr['sport'].append(corpus.data[i])\n",
    "    if(data==4):\n",
    "        arr['tech'].append(corpus.data[i])\n",
    "    i=i+1;\n",
    "\n",
    "total_business=0\n",
    "total_entertainment=0\n",
    "total_politics=0\n",
    "total_sport=0\n",
    "total_tech=0\n",
    "\n",
    "vectorizer2 = CountVectorizer()\n",
    "X = vectorizer2.fit_transform(arr['business'])\n",
    "business_arr=vectorizer2.vocabulary_\n",
    "for b in business_arr:\n",
    "    total_business+=vectorizer2.vocabulary_.get(b)\n",
    "\n",
    "vectorizer2 = CountVectorizer()\n",
    "X = vectorizer2.fit_transform(arr['entertainment'])\n",
    "entertainment_arr=vectorizer2.vocabulary_\n",
    "for e in entertainment_arr:\n",
    "    total_entertainment+=vectorizer2.vocabulary_.get(e)\n",
    "    \n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['politics'])\n",
    "politics_arr=vectorizer2.vocabulary_\n",
    "for p in politics_arr:\n",
    "    total_politics+=vectorizer2.vocabulary_.get(p)\n",
    "    \n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['sport'])\n",
    "sport_arr=vectorizer2.vocabulary_\n",
    "for s in sport_arr:\n",
    "    total_sport+=vectorizer2.vocabulary_.get(s)\n",
    "\n",
    "vectorizer2 = CountVectorizer()    \n",
    "X = vectorizer2.fit_transform(arr['tech'])\n",
    "tech_arr=vectorizer2.vocabulary_\n",
    "for t in tech_arr:\n",
    "    total_tech+=vectorizer2.vocabulary_.get(t)\n",
    "\n",
    "print(total_business)\n",
    "print(total_entertainment)\n",
    "print(total_politics)\n",
    "print(total_sport)\n",
    "print(total_tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f771fab",
   "metadata": {},
   "source": [
    "(h) the number of word-tokens in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088606d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e34a7e",
   "metadata": {},
   "source": [
    "(i) the number and percentage of words with a frequency of zero in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d50d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32a65688",
   "metadata": {},
   "source": [
    "(j) the number and percentage of words with a frequency of one in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ccd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10f1339f",
   "metadata": {},
   "source": [
    "(k) your 2 favorite words (that are present in the vocabulary) and their log-prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1e3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b6e15fc",
   "metadata": {},
   "source": [
    "Write to `bbc-performance.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfb467fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.write('\\n')\n",
    "f.write('\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('\\n')\n",
    "f.write('\\n')\n",
    "f.write('MultinomialNB with smoothing = 0.9\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('b) Confusion matrix\\n')\n",
    "f.write(DataFrame(cmatrix).to_string() + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('c) Classification Report\\n\\n')\n",
    "f.write(classificationReport + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('d) Accuracy, Macro Average F1, Weighted Average\\n')\n",
    "f.write('accuracy: ' + str(accuracyScore) + '\\n')\n",
    "f.write('macro average f1: ' + str(macroF1Score) + '\\n')\n",
    "f.write('weighted average: ' + str(weightedF1Score) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('e) Prior Probabilities\\n')\n",
    "f.write('business: ' + str(priors[3]) + '\\n')\n",
    "f.write('entertainment: ' + str(priors[1]) + '\\n')\n",
    "f.write('politics: ' + str(priors[2]) + '\\n')\n",
    "f.write('sports: ' + str(priors[4]) + '\\n')\n",
    "f.write('tech: ' + str(priors[0]) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('f) Size of vocabulary\\n')\n",
    "f.write('size of vocabulary: ' + str(vocabularyLength) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('g) Number of words-tokens in each class\\n')\n",
    "f.write('business: ' + str(total_business) + '\\n')\n",
    "f.write('entertainment: ' + str(total_entertainment) + '\\n')\n",
    "f.write('politics: ' + str(total_politics) + '\\n')\n",
    "f.write('sport: ' + str(total_sport) + '\\n')\n",
    "f.write('tech: ' + str(total_tech) + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('h) Number of words-tokens in entire corpus\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('i) Number and percentage of words with a frequency of zero in each class\\n')\n",
    "f.write('business: ' + '\\n')\n",
    "f.write('entertainment: ' + '\\n')\n",
    "f.write('politics: ' + '\\n')\n",
    "f.write('sport: ' + '\\n')\n",
    "f.write('tech: ' + '\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('j) Number and percentage of words with a frequency of one in the entire corpus\\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.write('k) Our 2 favorite words and their log-prob \\n')\n",
    "f.write('---------------------------------------------------------------------------------\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dda74d",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b6bc0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68ce9f",
   "metadata": {},
   "source": [
    "2. Load the dataset in Python (you can use pandas.read csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3d2e4f90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Na_to_K</th>\n",
       "      <th>Drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>25.355</td>\n",
       "      <td>drugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>13.093</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>10.114</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>F</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>7.798</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>18.043</td>\n",
       "      <td>drugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>56</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>11.567</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>12.006</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>52</td>\n",
       "      <td>M</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>9.894</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>14.020</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>11.349</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age Sex      BP Cholesterol  Na_to_K   Drug\n",
       "0     23   F    HIGH        HIGH   25.355  drugY\n",
       "1     47   M     LOW        HIGH   13.093  drugC\n",
       "2     47   M     LOW        HIGH   10.114  drugC\n",
       "3     28   F  NORMAL        HIGH    7.798  drugX\n",
       "4     61   F     LOW        HIGH   18.043  drugY\n",
       "..   ...  ..     ...         ...      ...    ...\n",
       "195   56   F     LOW        HIGH   11.567  drugC\n",
       "196   16   M     LOW        HIGH   12.006  drugC\n",
       "197   52   M  NORMAL        HIGH    9.894  drugX\n",
       "198   23   M  NORMAL      NORMAL   14.020  drugX\n",
       "199   40   F     LOW      NORMAL   11.349  drugX\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('drug200.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96d3ef",
   "metadata": {},
   "source": [
    "3. Plot the distribution of the instances in each class and store the graphic in a file called drug-distribution.pdf. You can use matplotlib.pyplot. This pre-analysis will allow you to determine if the classes are balanced,and which metric is more appropriate to use to evaluate the performance of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bef55818",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=np.array(df['Drug'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5b62f0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "instances=[None] * len(classes)\n",
    "index=0\n",
    "for drug in classes:\n",
    "    instances[index]=np.array(len(df['Drug'].loc[df['Drug']==drug]))\n",
    "    index+=1\n",
    "    \n",
    "drugFig = plt.figure()\n",
    "plt.bar(classes,instances)\n",
    "drugFig.savefig('drug-distribution.pdf', dpi=drugFig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee0586",
   "metadata": {},
   "source": [
    "4. Convert all ordinal and nominal features in numerical format. Make sure that your converted format respects the ordering of ordinal features, and does not introduce any ordering for nominal features. You may want to take a look at pandas.get dummies and pandas. Categorical to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7ab3d6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>...</th>\n",
       "      <th>31.686</th>\n",
       "      <th>31.876</th>\n",
       "      <th>32.922</th>\n",
       "      <th>33.486</th>\n",
       "      <th>33.542</th>\n",
       "      <th>34.686</th>\n",
       "      <th>34.997</th>\n",
       "      <th>35.639</th>\n",
       "      <th>37.188</th>\n",
       "      <th>38.247</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   F  M  15  16  17  18  19  20  21  22  ...  31.686  31.876  32.922  33.486  \\\n",
       "0  1  0   0   0   0   0   0   0   0   0  ...       0       0       0       0   \n",
       "1  0  1   0   0   0   0   0   0   0   0  ...       0       0       0       0   \n",
       "2  0  1   0   0   0   0   0   0   0   0  ...       0       0       0       0   \n",
       "3  1  0   0   0   0   0   0   0   0   0  ...       0       0       0       0   \n",
       "4  1  0   0   0   0   0   0   0   0   0  ...       0       0       0       0   \n",
       "\n",
       "   33.542  34.686  34.997  35.639  37.188  38.247  \n",
       "0       0       0       0       0       0       0  \n",
       "1       0       0       0       0       0       0  \n",
       "2       0       0       0       0       0       0  \n",
       "3       0       0       0       0       0       0  \n",
       "4       0       0       0       0       0       0  \n",
       "\n",
       "[5 rows x 316 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sex = pd.get_dummies(df['Sex'])\n",
    "df_new = sex\n",
    "\n",
    "age = pd.get_dummies(df['Age'])\n",
    "df_new = pd.concat([df_new, age], axis=1)\n",
    "\n",
    "bp = pd.get_dummies(df['BP'])\n",
    "df_new = pd.concat([df_new, age], axis=1)\n",
    "\n",
    "chol = pd.get_dummies(df['Cholesterol'])\n",
    "df_new = pd.concat([df_new, chol], axis=1)\n",
    "\n",
    "nak = pd.get_dummies(df['Na_to_K'])\n",
    "df_new = pd.concat([df_new, nak], axis=1)\n",
    "\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc04df4",
   "metadata": {},
   "source": [
    "5. Split the dataset using train test split using the default parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b70a676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_new\n",
    "y=df['Drug']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24eaa2",
   "metadata": {},
   "source": [
    "6. Run 6 different classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ce4ac",
   "metadata": {},
   "source": [
    "(a) NB: a Gaussian Naive Bayes Classifier (naive bayes.GaussianNB) with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "41abadcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['drugB', 'drugX', 'drugB', 'drugY', 'drugX', 'drugA', 'drugY',\n",
       "       'drugC', 'drugX', 'drugA', 'drugX', 'drugA', 'drugC', 'drugX',\n",
       "       'drugX', 'drugB', 'drugX', 'drugA', 'drugB', 'drugC', 'drugC',\n",
       "       'drugX', 'drugX', 'drugA', 'drugC', 'drugC', 'drugC', 'drugB',\n",
       "       'drugY', 'drugA', 'drugA', 'drugX', 'drugY', 'drugA', 'drugC',\n",
       "       'drugB', 'drugX', 'drugB', 'drugB', 'drugX'], dtype='<U5')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693278e4",
   "metadata": {},
   "source": [
    "(b) Base-DT: a Decision Tree (tree.DecisionTreeClassifier) with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "20135220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['drugY', 'drugY', 'drugY', 'drugY', 'drugY', 'drugY', 'drugY',\n",
       "       'drugY', 'drugY', 'drugY', 'drugY', 'drugA', 'drugC', 'drugY',\n",
       "       'drugX', 'drugY', 'drugY', 'drugY', 'drugX', 'drugY', 'drugY',\n",
       "       'drugY', 'drugY', 'drugY', 'drugC', 'drugY', 'drugY', 'drugY',\n",
       "       'drugY', 'drugY', 'drugY', 'drugY', 'drugY', 'drugY', 'drugC',\n",
       "       'drugY', 'drugY', 'drugX', 'drugB', 'drugY'], dtype=object)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dclf = DecisionTreeClassifier(random_state=0)\n",
    "dclf.fit(X_train, y_train)\n",
    "val_pred = dclf.predict(X_test)\n",
    "val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df043768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "110646fc04d8eb4b472f783b11b2cd66382864c9ecb559c878e82b087fcce6fc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
